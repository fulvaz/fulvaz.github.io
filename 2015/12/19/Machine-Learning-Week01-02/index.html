<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="author" content="Fulvaz"><meta name="description" content="我只是个混github连击的"><title>Machine Learning | Week01 &amp; 02 | Fulvaz PlayGroud</title><link href="/favicon.png" rel="icon"><link rel="alternate" href="/atom.xml" title="Fulvaz PlayGroud" type="application/atom.xml"><link href="http://fonts.googleapis.com/css?family=Open+Sans:400" rel="stylesheet"><link href="http://fonts.googleapis.com/css?family=Source+Code+Pro:400,600" rel="stylesheet" type="text/css"><link rel="stylesheet" href="/stylesheets/app.css"></head><body><section id="top-head"><div id="top-head-wraper"><a href="/">Fulvaz PlayGroud</a><div id="top-head-menu-button"></div></div></section><header id="page-header"><div class="header-background-container"><video loop="true" mute="true" autoplay="true" poster="http://7xp7jw.com1.z0.glb.clouddn.com/static/myTheme//images/header-background.jpg" class="header-background-video"><source src="http://7xp7jw.com1.z0.glb.clouddn.com/static/myTheme//videos/Blurry-Lights.webm" type="video/webm"/><source src="http://7xp7jw.com1.z0.glb.clouddn.com/static/myTheme//videos/Blurry-Lights.mp4" type="video/mp4"/><source src="http://7xp7jw.com1.z0.glb.clouddn.com/static/myTheme//videos/Blurry-Lights.ogv" type="video/ogv"/></video><div class="header-background-shelter"></div></div><div id="masthead"><div class="wrapper"><h1 id="site-title"><a href="/">Fulvaz PlayGroud</a></h1><p id="description">我只是个混github连击的</p></div></div></header><section id="content"><article class="post"><div class="article-head"><h2 class="article-title">Machine Learning | Week01 &amp; 02</h2><div class="meta article-date">2015-12-19</div></div><div class="article-content"><p><h1 id="Week-01"><a href="#Week-01" class="headerlink" title="Week 01"></a>Week 01</h1><hr>
<h1 id="supervisor-learning"><a href="#supervisor-learning" class="headerlink" title="supervisor learning"></a>supervisor learning</h1><p>are given a data set and already know what our correct output should look like<br>unsupervise leanring</p>
<h2 id="regression-problem"><a href="#regression-problem" class="headerlink" title="regression problem"></a>regression problem</h2><p>predict result with <strong>continuous</strong> output</p>
<h2 id="classification-problem"><a href="#classification-problem" class="headerlink" title="classification problem"></a>classification problem</h2><p>predict in <strong>discrete</strong> categories</p>
<h1 id="unsupervisor-learning"><a href="#unsupervisor-learning" class="headerlink" title="unsupervisor learning"></a>unsupervisor learning</h1><p>我们有一堆数据,但是我们自己也不知道会产生什么结果,我们用算法produce结果</p>
<p>关键: clustering<br>收集相关的数据</p>
<h1 id="Linear-Regression-with-One-Variable"><a href="#Linear-Regression-with-One-Variable" class="headerlink" title="Linear Regression with One Variable"></a>Linear Regression with One Variable</h1><h2 id="The-Hypothesis-Function"><a href="#The-Hypothesis-Function" class="headerlink" title="The Hypothesis Function"></a>The Hypothesis Function</h2><h2 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h2><p>描述hypothesis function准确度的函数<br><img src="https://leanote.com/api/file/getImage?fileId=566e3d76ab64416467000ef9" alt="">(<a href="https://leanote.com/api/file/getImage?fileId=5667cab8ab6441601e0014be" target="_blank" rel="external">https://leanote.com/api/file/getImage?fileId=5667cab8ab6441601e0014be</a>)</p>
<p>这函数叫<code>&quot;Squared error function&quot;</code>, or <code>Mean squared error</code></p>
<p>1/2的原因是方便这个函数后面用梯度下降法(computation of the gradient descent), 因为平方的倒数项的导数的常数刚好和1/2约掉(as the derivative term of the square function will cancel out the 12 term.) &lt;- 学英文</p>
<p> taking the derivative 求导</p>
<h1 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h1><p>way to automatically improve our hypothesis function<br>repeat until convergence:<br><img src="https://leanote.com/api/file/getImage?fileId=566e3d76ab64416467000ef8" alt="">(<a href="https://leanote.com/api/file/getImage?fileId=5667cab8ab6441601e0014ba" target="_blank" rel="external">https://leanote.com/api/file/getImage?fileId=5667cab8ab6441601e0014ba</a>)</p>
<p><code>:=</code> 的意思是assignment, 即编程里面的<code>=</code>, 因为数学不能直接使用<code>=</code></p>
<p>没有必要修改alpha(learning rate), 取导的时候会,整项会自动变小</p>
<h1 id="Gradient-Descent-for-Linear-Regression"><a href="#Gradient-Descent-for-Linear-Regression" class="headerlink" title="Gradient Descent for Linear Regression"></a>Gradient Descent for Linear Regression</h1><p>经过一番不为人知的推导,得出来优化过的公式<br><img src="https://leanote.com/api/file/getImage?fileId=566e3d75ab64416467000ef3" alt="">(<a href="https://leanote.com/api/file/getImage?fileId=5667cab8ab6441601e0014c4" target="_blank" rel="external">https://leanote.com/api/file/getImage?fileId=5667cab8ab6441601e0014c4</a>)</p>
<p><a href="https://eventing.coursera.org/api/redirectStrict/QgxGOfcpet0RTRhoSKJ1ezxyBz8TJelio7I-8zayGbedVqU0SZji70rZzgY-owQoCsgg6gt5y0fNjYbB7_zidg.YokNaWe3vKM8XhRLSFOV3w.OhCCAk2hbbjCOeheHZT-Ec3YODIG0LI8bEMdOFB_X_yR99_Xuew4jF2nlOlWJVyF8KivhfQENCP2_HU2NKHgAw_EjHVO7WZcn-vhXG2Ai61Ge99NadDaU-01Z6JC93LlkFSxP4un70pso_XiE9k4uGIBI2hfXhiObFrwJh5hkD3c5P1RgoPfOKRyXSljc49sAOJ44WTEeRBSnwhuKGUHVs1tHeUU3ac0f9-ksLQMCd6uY4rYHAYFAlwSAXnVgf_s6qTGVMm75DvAhDPhvjroKczVk9hgnZ-DjxXkRahjpS0CMTZLbI0pmfBZ4VKrj7SXGbdYLfpVOOcnFA93YK8ifur1QrxqArZgRKkU9RqtB9HesH18w1814VumDEPz0Iry2xxzodp6OH8bLG5rCkRXzF6N9POexZFjZiCRCvvshldQlHY3juvTwk02OH9BtkbOH6MzeGBMU4ki9thpIG1kVXnP_TwjGwdmGTfmO1gdTOa3imoZU79WZXvL2gSRS84NmPpCBopOH8QScrCLnMqkuA" target="_blank" rel="external">推导过程</a>在这里,但是我没勇气去看</p>
<h1 id="线性代数复习"><a href="#线性代数复习" class="headerlink" title="线性代数复习"></a>线性代数复习</h1><h2 id="Notation-and-terms"><a href="#Notation-and-terms" class="headerlink" title="Notation and terms:"></a>Notation and terms:</h2><ul>
<li><p>Aij refers to the element in the ith row and jth column of matrix A.</p>
</li>
<li><p>A vector with ‘n’ rows is referred to as an ‘n’-dimensional vector</p>
</li>
<li><p>vi refers to the element in the ith row of the vector.</p>
</li>
<li><p>In general, all our vectors and matrices will be 1-indexed.</p>
</li>
<li><p>Matrices are usually denoted by uppercase names while vectors are lowercase.</p>
</li>
<li><p>“Scalar” means that an object is a single value, not a vector or matrix.</p>
</li>
<li><p>R refers to the set of scalar real numbers</p>
</li>
<li><p>Rn refers to the set of n-dimensional vectors of real numbers</p>
</li>
</ul>
<h2 id="向量与矩阵"><a href="#向量与矩阵" class="headerlink" title="向量与矩阵"></a>向量与矩阵</h2><p>小写表示向量Vector<br>大写表示矩阵matrix</p>
<h2 id="矩阵与代数"><a href="#矩阵与代数" class="headerlink" title="矩阵与代数"></a>矩阵与代数</h2><p>看图<br><img src="https://leanote.com/api/file/getImage?fileId=566e3d76ab64416467000efb" alt="">(<a href="https://leanote.com/api/file/getImage?fileId=566839fcab6441616d001916" target="_blank" rel="external">https://leanote.com/api/file/getImage?fileId=566839fcab6441616d001916</a>)</p>
<h2 id="Identity-matrix"><a href="#Identity-matrix" class="headerlink" title="Identity matrix"></a>Identity matrix</h2><p>对角全是1,其他0..中文叫啥来着<br>A<em>I = I</em>A = A</p>
<p>inverse<br>-<br>A^-1<br>only square matrix has inverse matrix</p>
<p>A * A^-1 = I</p>
<h1 id="week02-Multivariate-Linear-Regression"><a href="#week02-Multivariate-Linear-Regression" class="headerlink" title="week02 Multivariate Linear Regression"></a>week02 Multivariate Linear Regression</h1><hr>
<p>hypothesis: n特征,n个参数</p>
<h2 id="Gradient-Descent-for-Multiple-Variables"><a href="#Gradient-Descent-for-Multiple-Variables" class="headerlink" title="Gradient Descent for Multiple Variables"></a>Gradient Descent for Multiple Variables</h2><h2 id="Feature-Scaling"><a href="#Feature-Scaling" class="headerlink" title="Feature Scaling"></a>Feature Scaling</h2><p>如果多个特征中,每个特征的样本之间量级差别特别大,其收敛速度会非常慢</p>
<p>考虑归一化,使converge收敛更快<br><img src="https://leanote.com/api/file/getImage?fileId=566e3d76ab64416467000ef4" alt="">(<a href="https://leanote.com/api/file/getImage?fileId=566839fcab6441616d001912" target="_blank" rel="external">https://leanote.com/api/file/getImage?fileId=566839fcab6441616d001912</a>)</p>
<p>尽量使x在[-1, 1], 当然是个经验值</p>
<p>###Mean normalization<br><img src="https://leanote.com/api/file/getImage?fileId=566e3d76ab64416467000ef7" alt="">(<a href="https://leanote.com/api/file/getImage?fileId=566839fcab6441616d001917" target="_blank" rel="external">https://leanote.com/api/file/getImage?fileId=566839fcab6441616d001917</a>)</p>
<h2 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h2><p>if not work, use smaller alpha</p>
<ul>
<li>足够小的alpha,J(theta)会在每次迭代中变小</li>
<li>太小的alpha会收敛很慢</li>
</ul>
<p>###如何找到alpha<br>尝试<br>….0.001, …0.01,… 0.1, ….1</p>
<h2 id="Features-and-Polynomial-Regression"><a href="#Features-and-Polynomial-Regression" class="headerlink" title="Features and Polynomial Regression"></a>Features and Polynomial Regression</h2><p>选择正确的<strong>model(feature)</strong></p>
<p>比如房子, w*d(size)一个特征, 比两个w, d要好</p>
<p>还要<strong>hypothesis的选择</strong><br>比如说房价, x^3…比x^2会好,因为x^2上升到一定程度房价反而下降了,这是不可能的是吧</p>
<p>要根据实际情况选择</p>
<p>值得一提的是,如果多项式次数比较多,归一化会非常重要</p>
<h2 id="Normal-Equation"><a href="#Normal-Equation" class="headerlink" title="Normal Equation"></a>Normal Equation</h2><p>求J方法<br><strong>1.求导</strong><br>但如果theta不是实数就跪在了地上</p>
<p><strong>2.矩阵求θ</strong><br>$θ=(X^TX)^{-1}X^T$     </p>
<p><img src="https://leanote.com/api/file/getImage?fileId=566e3d76ab64416467000ef6" alt=""></p>
<p>ps:如果你用normal equation, feature scaling就看可以不加了</p>
<p><strong>pro&amp;cons:</strong><br><img src="https://leanote.com/api/file/getImage?fileId=566e3d76ab64416467000ef5" alt="">(<a href="https://leanote.com/api/file/getImage?fileId=566942e1ab64413e67000579" target="_blank" rel="external">https://leanote.com/api/file/getImage?fileId=566942e1ab64413e67000579</a>)</p>
<p>###what if $XX^T$ is non-invertible?<br>原因:</p>
<ol>
<li>有冗余数据</li>
<li>feature太多</li>
</ol>
<h2 id="Multivariate-linear-regression"><a href="#Multivariate-linear-regression" class="headerlink" title="Multivariate linear regression"></a>Multivariate linear regression</h2><p>算θ的通用公式<br><img src="https://leanote.com/api/file/getImage?fileId=566e3d76ab64416467000efa" alt="">(<a href="https://leanote.com/api/file/getImage?fileId=566e3d76ab64416467000efc" target="_blank" rel="external">https://leanote.com/api/file/getImage?fileId=566e3d76ab64416467000efc</a>)</p>
<h1 id="Week02-Octave-tutoria"><a href="#Week02-Octave-tutoria" class="headerlink" title="Week02 Octave tutoria"></a>Week02 Octave tutoria</h1><p>load<br>save a.mat a</p>
<p>A(3, 2)<br>A(2, :) %取出第二行的内容<br>A(:, 2)<br>A([1 3], :) %取第1,3行内容<br>A = [A, [100; 101; 102]] %右边添加一列<br>A(:) %全部数据放一行<br>C = [A; B] %A下面B<br>C = [A B] %A右边B<br>eye identity matrix</p>
<h2 id="ComputingData"><a href="#ComputingData" class="headerlink" title="ComputingData"></a>ComputingData</h2><p>A <em> B %矩阵相乘<br>A .</em>B %分别相乘 <code>.</code>是元素级别的操作<br>abs(A)<br>v+ ones(length(v), 1)<br>A<code>%tanspose
(A</code>)` %tanspose and transpose<br>max(a)<br>a &lt; 3<br>find(a &lt; 3)<br>A = magic(3) %generate a 3*3 which have the same resule<br>sum(a) %将a全部元素加起来<br>prod<br>floor<br>ceil<br>sum(A, 1) ssu up eacch col<br>sum(A, 2) sum up each row<br>pinv %inverse a matrix</p>
<h2 id="ploting-data"><a href="#ploting-data" class="headerlink" title="ploting data"></a>ploting data</h2><p>plot<br>hold on %plot in the same window<br>xlabel<br>ylabel<br>legend<br>title<br>print -dpng \’ex.png\’<br>figure(1); plot %plot in different window<br>subplot(1, 2, 1) %create 1*2 grid and focus on the first elememt, use subplot again to draw the following plot<br>axis([0.5 1 -1 1])<br>imageesc(A) % eh….<br>colorbar<br>colormap gray</p>
<h2 id="Control-statement"><a href="#Control-statement" class="headerlink" title="Control statement"></a>Control statement</h2><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%<span class="keyword">for</span> <span class="keyword">loop</span></span><br><span class="line"><span class="keyword">for</span> i=<span class="number">1</span>:<span class="number">10</span>, </span><br><span class="line">    </span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure>
<p>while<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="built_in">i</span> ＜＝５</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>disp % == print!</p>
<p>###define your own function<br>cd to the path of source code<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% source.m</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[y1, y2]</span> = <span class="title">name</span><span class="params">(params)</span></span></span><br><span class="line">y1 = x^<span class="number">2</span></span><br><span class="line">y2 = x^<span class="number">3</span></span><br></pre></td></tr></table></figure></p>
<h2 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h2><p>矩阵运算比for loop快得多,而且代码量少很多 (代码少,bug也就少</p>
<p>###example<br><img src="https://leanote.com/api/file/getImage?fileId=5670404fab64416467001cdd" alt="">(<a href="https://leanote.com/api/file/getImage?fileId=5670404fab64416467001cdf" target="_blank" rel="external">https://leanote.com/api/file/getImage?fileId=5670404fab64416467001cdf</a>)</p>
<p>###gredient descent<br><img src="https://leanote.com/api/file/getImage?fileId=5670404fab64416467001cde" alt="">(<a href="https://leanote.com/api/file/getImage?fileId=5670404fab64416467001cdc" target="_blank" rel="external">https://leanote.com/api/file/getImage?fileId=5670404fab64416467001cdc</a>)<br>其实就是将求导运算转换成了矩阵运算</p>
</p></div></article></section><nav id="article-list-pagination"><a href="/2015/12/19/Machine-Learning-Week3/" title="上一页 article: Machine Learning Week3" class="prev">&larr; 上一页</a><a href="/2015/12/17/硬盘被误操作格了之后/" title="下一页 article: 硬盘被误操作格了之后" class="next">下一页 &rarr;</a></nav><footer id="page-footer"><section><nav id="footer-menu"><ul><li><a href="/about">About</a></li><li><a href="/archives">Archive</a></li><li><a href="https://github.com/subuta/hexo-jade-sass-barebone">Source</a></li></ul></nav></section></footer></body></html>